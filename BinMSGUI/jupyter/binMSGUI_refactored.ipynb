{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import struct\n",
    "import json\n",
    "import datetime\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "notebook_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# the code path is two folders up from this notebook + /code\n",
    "core_path = os.path.dirname(notebook_path)\n",
    "basepath = os.path.dirname(os.path.dirname(notebook_path))\n",
    "\n",
    "sys.path.append(core_path)\n",
    "sys.path.append(basepath)\n",
    "\n",
    "from core.readMDA import readMDA, get_Fs\n",
    "from core.readBin import get_bin_data, get_raw_pos, get_active_tetrode, get_channel_from_tetrode, get_active_eeg\n",
    "from core.Tint_Matlab import int16toint8, get_setfile_parameter\n",
    "from core.tetrode_conversion import convert_tetrode, batch_basename_tetrodes\n",
    "from core.convert_position import convert_position\n",
    "from core.eeg_conversion import convert_eeg\n",
    "from core.utils import find_sub, find_bin_basenames\n",
    "from core.bin2mda import convert_bin2mda\n",
    "from core.mdaSort import sort_bin\n",
    "from core.set_conversion import convert_setfile\n",
    "from core.intan_mountainsort import validate_session, convert_bin_mountainsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 basenames within this directory: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\n",
      "------------------------\n",
      "20190117-1150-1\n"
     ]
    }
   ],
   "source": [
    "# directory that you want to analyze\n",
    "directory = 'E:\\\\Apollo_D_Drive\\\\ApolloKlusta\\\\J20-sleep-1'\n",
    "directory = 'E:\\\\Apollo_D_Drive\\\\ApolloKlusta\\\\test_mountainsort'\n",
    "#directory = 'E:\\\\Apollo_D_Drive\\\\data\\\\B6-August-18-1'\n",
    "\n",
    "# finds the basenames within this directory\n",
    "basenames = find_bin_basenames(directory)\n",
    "\n",
    "# prints all the basenames\n",
    "print('Found %d basenames within this directory: %s' % (len(basenames), directory))\n",
    "print('------------------------')\n",
    "for name in basenames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiten = 'true'  # do you want to whiten the data?\n",
    "# whiten = 'false'\n",
    "\n",
    "# detect_interval = 50 \n",
    "detect_interval = 10  # roughly the number of samples to check for a spike\n",
    "# the algorithm will take the detect_interval value and bin the data in bin sizes of that many\n",
    "# samples. Then it will find the peak (or trough, or both) of each bin and evaluate that event\n",
    "# if it exceeds the threshold value.\n",
    "\n",
    "# recommend only doing positive peaks so we don't get any weird issues with a cell that is\n",
    "# aligned with the peak, and seemingly the same cell aligned with the trough (in this case\n",
    "# both peak and trough would have to exceed the threshold).\n",
    "\n",
    "# detect_sign = 0  # positive or negative peaks\n",
    "detect_sign = 1  # only positive peaks\n",
    "# detect_sign = -1  # only negative peaks\n",
    "\n",
    "# threshold values, I changed it into a whitened and non whitened threshold\n",
    "# this is because if you whiten the data you normalize it by the variance, thus\n",
    "# a threshold of 3 is essentially saying 3 standard deviations. However if you do not whiten\n",
    "# the data is not normalized and thus, you would be using a bit value, maybe should take whatever\n",
    "# value is in the threshold from the set file.\n",
    "\n",
    "if whiten == 'true':\n",
    "    detect_threshold = 3  # units: ~sd's\n",
    "    # detect_threshold = 4  # units: ~sd's\n",
    "    # ---------------\n",
    "    automate_threshold = False  # Don't Change this\n",
    "    \n",
    "else:\n",
    "    # this mean's the data was not whitened\n",
    "    \n",
    "    detect_threshold = 13000  #  units: bits \n",
    "    \n",
    "    # if you want to find the threshold from the .set file and use that \n",
    "    # set automate_threshold to True, otherwise False. This threshold would override any\n",
    "    # value set above. I'd recommend setting this to true as this is variable from .set file\n",
    "    # to .set file it seems.\n",
    "    # automate_threshold = True \n",
    "    automate_threshold = False\n",
    "    \n",
    "# bandpass filtering parameters, don't really know this\n",
    "freq_min = 300  # this doesn't really matter because data is already filtered so it won't do the filtering\n",
    "freq_max = 7000  # this doesn't really matter because data is already filtered so it won't do the filtering\n",
    "\n",
    "pre_spike = 15\n",
    "post_spike = 35\n",
    "\n",
    "# artifact masking parameters\n",
    "# here we bin the data into masked_chunk_size bins, and it will take the sqrt of the sum of \n",
    "# the squares (RSS) for each bin. It will then find the SD for all the bins, and if the bin is\n",
    "# above mask_threshold SD's from the average bin RSS, it will consider it as high amplitude noise\n",
    "# and remove this chunk (and neighboring chunks).\n",
    "mask = True\n",
    "mask = False\n",
    "mask_threshold = 6  #  units: SD's\n",
    "masked_chunk_size = None  # if none it will default to Fs/10\n",
    "mask_num_write_chunks = 100  # \n",
    "\n",
    "# feature parameters\n",
    "num_features = 15\n",
    "max_num_clips_for_pca = 1100\n",
    "\n",
    "# random parameters, probably don't need to change\n",
    "\n",
    "clip_size = 50  # this needs to be left at 50 for Tint, Tint only likes 50 samples\n",
    "notch_filter = False  # the data is already notch filtered likely\n",
    "self = None  # don't worry about this, this is for objective oriented programming (my GUIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs Analysis on each Basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing set_file 1/1: \n",
      "Using the following detect_threshold: 3.00\n",
      "[2019-02-01 15:10:12]: The following set file has already been created: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms.set, skipping creation!#Red\n",
      "[2019-02-01 15:10:12]: Converting the following tetrode: 1!\n",
      "[2019-02-01 15:10:12]: The following filename already exists: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T1_filt.mda, skipping conversion!#Red\n",
      "[2019-02-01 15:10:12]: Converting the following tetrode: 2!\n",
      "[2019-02-01 15:10:12]: The following filename already exists: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T2_filt.mda, skipping conversion!#Red\n",
      "[2019-02-01 15:10:12]: Converting the following tetrode: 3!\n",
      "[2019-02-01 15:10:12]: The following filename already exists: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T3_filt.mda, skipping conversion!#Red\n",
      "[2019-02-01 15:10:12]: Converting the following tetrode: 4!\n",
      "[2019-02-01 15:10:12]: The following filename already exists: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T4_filt.mda, skipping conversion!#Red\n",
      "15 1100\n",
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T1_filt.mda --outputs firings_out:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T1_firings.mda pre_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T1_pre.mda metrics_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T1_metrics.json --parameters freq_min:300 freq_max:7000 samplerate:48000 detect_sign:1 adjacency_radius:-1 detect_threshold:3 detect_interval:10 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:false mask_chunk_size:4800 mask_threshold:6 mask_num_write_chunks:100 num_workers:12 whiten:true num_features:15 max_num_clips_for_pca:1100 >> /mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T1_terminal.txt\n",
      "15 1100\n",
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T2_filt.mda --outputs firings_out:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T2_firings.mda pre_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T2_pre.mda metrics_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T2_metrics.json --parameters freq_min:300 freq_max:7000 samplerate:48000 detect_sign:1 adjacency_radius:-1 detect_threshold:3 detect_interval:10 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:false mask_chunk_size:4800 mask_threshold:6 mask_num_write_chunks:100 num_workers:12 whiten:true num_features:15 max_num_clips_for_pca:1100 >> /mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T2_terminal.txt\n",
      "15 1100\n",
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T3_filt.mda --outputs firings_out:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T3_firings.mda pre_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T3_pre.mda metrics_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T3_metrics.json --parameters freq_min:300 freq_max:7000 samplerate:48000 detect_sign:1 adjacency_radius:-1 detect_threshold:3 detect_interval:10 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:false mask_chunk_size:4800 mask_threshold:6 mask_num_write_chunks:100 num_workers:12 whiten:true num_features:15 max_num_clips_for_pca:1100 >> /mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T3_terminal.txt\n",
      "15 1100\n",
      "ml-run-process ms4_geoff.sort --inputs filt_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T4_filt.mda --outputs firings_out:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T4_firings.mda pre_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T4_pre.mda metrics_out_fname:/mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T4_metrics.json --parameters freq_min:300 freq_max:7000 samplerate:48000 detect_sign:1 adjacency_radius:-1 detect_threshold:3 detect_interval:10 clip_size:50 firing_rate_thresh:0.05 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5 mask_artifacts:false mask_chunk_size:4800 mask_threshold:6 mask_num_write_chunks:100 num_workers:12 whiten:true num_features:15 max_num_clips_for_pca:1100 >> /mnt/e/Apollo_D_Drive/ApolloKlusta/test_mountainsort/20190117-1150-1_T4_terminal.txt\n",
      "[2019-02-01 15:20:40]: Analyzing the following bin file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1.bin!\n",
      "[2019-02-01 15:20:40]: Reading in the position data!\n",
      "[2019-02-01 15:20:46]: Creating the .pos file!\n",
      "[2019-02-01 15:20:46]: Converting the MountainSort output following filename to Tint: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T1_filt.mda\n",
      "[2019-02-01 15:20:46]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T1_firings.mda\n",
      "[2019-02-01 15:20:46]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T1_firings.mda\n",
      "[2019-02-01 15:21:04]: Creating the following tetrode file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms.1!\n",
      "[2019-02-01 15:21:11]: Creating the following cut file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms_1.cut!\n",
      "[2019-02-01 15:21:15]: Converting the MountainSort output following filename to Tint: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T2_filt.mda\n",
      "[2019-02-01 15:21:15]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T2_firings.mda\n",
      "[2019-02-01 15:21:15]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T2_firings.mda\n",
      "[2019-02-01 15:21:33]: Creating the following tetrode file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms.2!\n",
      "[2019-02-01 15:21:41]: Creating the following cut file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms_2.cut!\n",
      "[2019-02-01 15:21:45]: Converting the MountainSort output following filename to Tint: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T3_filt.mda\n",
      "[2019-02-01 15:21:45]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T3_firings.mda\n",
      "[2019-02-01 15:21:45]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T3_firings.mda\n",
      "[2019-02-01 15:22:05]: Creating the following tetrode file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms.3!\n",
      "[2019-02-01 15:22:09]: Creating the following cut file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms_3.cut!\n",
      "[2019-02-01 15:22:11]: Converting the MountainSort output following filename to Tint: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T4_filt.mda\n",
      "[2019-02-01 15:22:11]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T4_firings.mda\n",
      "[2019-02-01 15:22:11]: Reading the spike data from the following file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_T4_firings.mda\n",
      "[2019-02-01 15:22:30]: Creating the following tetrode file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms.4!\n",
      "[2019-02-01 15:22:34]: Creating the following cut file: E:\\Apollo_D_Drive\\ApolloKlusta\\test_mountainsort\\20190117-1150-1_ms_4.cut!\n",
      "[2019-02-01 15:22:36]: Creating the following EEG file: .eeg!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\scipy\\signal\\signaltools.py:1344: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out = out_full[ind]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-02-01 15:22:57]: Creating the following EEG file: .eeg2!\n",
      "[2019-02-01 15:23:17]: Creating the following EEG file: .eeg3!\n",
      "[2019-02-01 15:23:39]: Creating the following EEG file: .eeg4!\n",
      "[2019-02-01 15:23:59]: Finished converting the following session: 20190117-1150-1!\n",
      "-------------------\n",
      "Finished Analysis\n"
     ]
    }
   ],
   "source": [
    "for i, current_basename in enumerate(basenames):\n",
    "    print('Analyzing set_file %d/%d: ' % (i+1, len(basenames)))\n",
    "    \n",
    "    if whiten != 'true' and automate_threshold:\n",
    "        # then you decided you want to automatically get the threshold from the .set file\n",
    "        set_filename = '%s.set' % os.path.join(directory, current_basename)\n",
    "        detect_threshold = int(get_setfile_parameter('threshold', set_filename))\n",
    "    \n",
    "    print('Using the following detect_threshold: %.2f' % (float(detect_threshold)))\n",
    "        \n",
    "    convert_bin_mountainsort(directory, current_basename, whiten=whiten, \n",
    "                             detect_interval=detect_interval,\n",
    "                             detect_sign=detect_sign, \n",
    "                             detect_threshold=detect_threshold, \n",
    "                             freq_min=freq_min,\n",
    "                             freq_max=freq_max, mask_threshold=mask_threshold, \n",
    "                             masked_chunk_size=masked_chunk_size,\n",
    "                             mask_num_write_chunks=mask_num_write_chunks, \n",
    "                             clip_size=clip_size, \n",
    "                             mask=mask,\n",
    "                             num_features=num_features,\n",
    "                             max_num_clips_for_pca=max_num_clips_for_pca,\n",
    "                             pre_spike=pre_spike, post_spike=post_spike,\n",
    "                             notch_filter=notch_filter, self=self)\n",
    "    \n",
    "    print('-------------------')\n",
    "    \n",
    "print('Finished Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
